<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">

  <title>Stanford CS 224N | Natural Language Processing with Deep Learning</title>
</head>

<body>
<!-- Content -->
<div class="container sec" id="content">
  <h2>Natural Language Processing with Deep Learning</h2>
  <li>
    <a href="index2019.html#schedule">CS224N-2019</a>
  </li>
  <li>
    <a href="https://web.stanford.edu/class/cs224n/index.html">CS224N</a>
  </li>
  <li>
    <a href="https://m.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z">Playlist</a>
  </li>

  <h3>What is this course about?</h3>
  <p>
    Natural language processing (NLP) or computational linguistics is one of the most important technologies of the
    information age.
    Applications of NLP are everywhere because people communicate almost everything in language: web search, advertising,
    emails, customer service, language translation, virtual agents, medical reports, politics, etc.
    In the 2010s, deep learning (or neural network) approaches obtained very high performance across many different NLP
    tasks, using single end-to-end neural models that did not require traditional, task-specific feature engineering. In
    the 2020s amazing further progress was made through the scaling of Large Language Models, such as ChatGPT. In this
    course, students will gain a thorough introduction to both the basics of Deep Learning for NLP and the latest
    cutting-edge research on Large Language Models (LLMs).
    Through lectures, assignments and a final project, students will learn the necessary skills to design, implement, and
    understand their own neural network models, using the <a href="https://pytorch.org/">Pytorch</a> framework.
  </p>
  <blockquote>
    <small>
      <i>“Take it. CS221 taught me algorithms. CS229 taught me math. CS224N taught me how to write machine learning models.”</i> – A CS224N student on Carta
    </small>
  </blockquote>
  <h3>Previous offerings</h3>
  <p>
    Below you can find archived websites and student project reports from previous years. <b><font color="red">Disclaimer: assignments change from year to year; please do not do assignments from previous years!</font></b>
  </p>
  <div>
    <table class="table">
      <tr class="active">
        <td>
            <b>CS224n Websites</b>:
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244">Winter 2024</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234">Winter 2023</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224">Winter 2022</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214">Winter 2021</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204">Winter 2020</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194">Winter 2019</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184">Winter 2018</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174">Winter 2017</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162">Autumn 2015</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1152">Autumn 2014</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1142">Autumn 2013</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1132">Autumn 2012</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1122">Autumn 2011</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1114">Winter 2011</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1106">Spring 2010</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1096">Spring 2009</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1086">Spring 2008</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1076">Spring 2007</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1066">Spring 2006</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1056">Spring 2005</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1046">Spring 2004</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1036">Spring 2003</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1026">Spring 2002</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/manning.499">Spring 2000</a>
        </td>
      </tr>
      <tr class="active">
        <td>
            <b>CS224n Lecture Videos</b>:
            <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4">Winter 2023</a> /
            <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ">Winter 2021</a> /
            <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z">Winter 2019</a> /
            <a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6">Winter 2017</a>
        </td>
      </tr>
      <tr class="active">
        <td>
            <b>CS224n Reports</b>:
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/project.html">Winter 2024</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/project.html">Winter 2023</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/project.html">Winter 2022</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/project.html">Winter 2021</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/project.html">Winter 2020</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/project.html">Winter 2019</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports.html">Winter 2018</a> /
            <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports.html">Winter 2017</a> /
            <a href="http://nlp.stanford.edu/courses/cs224n/">Autumn 2015 and earlier</a>
        </td>
      </tr>
      <tr class="active">
        <td>
            <b>CS224d Reports</b>:
            <a href="http://cs224d.stanford.edu/reports_2016.html">Spring 2016</a> /
            <a href="http://cs224d.stanford.edu/reports_2015.html">Spring 2015</a>
        </td>
      </tr>
    </table>
  </div>
  <h3>Prerequisites</h3>
  <ul>
      <li><b>Proficiency in Python</b>
          <p>All class assignments will be in Python (using <a href="https://numpy.org/">NumPy</a> and <a href="https://pytorch.org">PyTorch</a>). If you need to remind yourself of Python, or you're not very familiar with NumPy, you can come to the Python review session in week 1 (listed in the <a href="#schedule">schedule</a>). If you have a lot of programming experience but in a different language (e.g. C/C++/Matlab/Java/Javascript), you will probably be fine.</p>
      </li>
      <li><b>College Calculus, Linear Algebra</b> (e.g. MATH 51, CME 100)
          <p>You should be comfortable taking (multivariable) derivatives and understanding matrix/vector notation and operations.</p>
      </li>
      <li><b>Basic Probability and Statistics</b> (e.g. CS 109 or equivalent)
          <p>You should know the basics of probabilities, gaussian distributions, mean, standard deviation, etc.</p>
      </li>
      <li><b>Foundations of Machine Learning</b> (e.g. CS221, CS229, CS230, or CS124)
          <p>We will be formulating cost functions, taking derivatives and performing optimization with gradient descent.
            If you already have basic machine learning and/or deep learning knowledge, the course will be easier; however it is possible to take CS224n without it. There are many introductions to ML, in webpage, book, and video form. One approachable introduction is Hal Daum&eacute;’s in-progress <a href="http://ciml.info"><i>A Course in Machine Learning</i></a>. Reading the first 5 chapters of that book would be good background. Knowing the first 7 chapters would be even better!</p>
      </li>
  </ul>
  <h3>Reference Texts</h3>
  <p>
    The following texts are useful, but none are required. All of them can be read free online.
  </p>
  <ul>
      <li>
          Dan Jurafsky and James H. Martin. <a href="https://web.stanford.edu/~jurafsky/slpdraft/">Speech and Language Processing (2024 pre-release)</a>
    </li>
      <li>
          Jacob Eisenstein. <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Natural Language Processing</a>
      </li>
      <li>
          Yoav Goldberg. <a href="http://u.cs.biu.ac.il/~yogo/nnlp.pdf">A Primer on Neural Network Models for Natural Language Processing</a>
      </li>
      <li>
          Ian Goodfellow, Yoshua Bengio, and Aaron Courville. <a href="http://www.deeplearningbook.org/">Deep Learning</a>
      </li>
      <li>
          Delip Rao and Brian McMahan. <a href="http://library.stanford.edu/sfx?genre=book&atitle=&title=Natural%20language%20processing%20with%20PyTorch%20:%20build%20intelligent%20language%20applications%20using%20deep%20learning%20/&isbn=9781491978207&volume=&issue=&date=20190101&aulast=Rao,%20Delip,,%20author.&spage=&pages=&sid=EBSCO:VLeBooks:edsvle.AH35866319">Natural Language Processing with PyTorch</a> (requires Stanford login).
      </li>
      <li>
          Lewis Tunstall, Leandro von Werra, and Thomas Wolf. <a href="https://transformersbook.com/">Natural Language Processing with Transformers</a>
      </li>
    </ul>
    <p>
    If you have no background in neural networks but would like to take the course anyway, you might well find one of these books helpful to give you more background:
    </p>
    <ul>
      <li>
         Michael A. Nielsen. <a href="http://neuralnetworksanddeeplearning.com">Neural Networks and Deep Learning</a>
      </li>
      <li>
      Eugene Charniak. <a href="https://mitpress.mit.edu/books/introduction-deep-learning">Introduction to Deep Learning</a>
      </li>
  </ul>
</div>

<!-- Schedule -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<br>
<h2>Schedule</h2>
<p>
  Updated lecture <b>slides</b> will be posted here shortly before each lecture. Other links contain last year's slides, which are mostly similar.
</p>
<p>
  Lecture <b>notes</b> will be uploaded a few days after most lectures. The notes (which cover approximately the first half of the course content) give supplementary detail beyond the lectures.
</p>
<p>
  <b><font color="red">Disclaimer: Assignments change; please do not do old assignments. We will give no points for doing last year's assignments.</font></b>
</p>

<table class="table">
  <colgroup>
    <col style="width:10%">
    <col style="width:20%">
    <col style="width:40%">
    <col style="width:10%">
    <col style="width:10%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>Date</th>
    <th>Description</th>
    <th>Course Materials</th>
    <th>Events</th>
    <th>Deadlines</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td><b><font color="#8c1515">Week 1</font></b><br><br>Tue Apr 2</td>
    <td>Word Vectors
      <br>
      [<a href="slides/cs224n-spr2024-lecture01-wordvecs1.pdf">slides</a>]
      [<a href="readings/cs224n_winter2023_lecture1_notes_draft.pdf">notes</a>]
      <!--<br><br>
      Gensim word vectors example:
      <br>
      [<a href="materials/Gensim.zip">code</a>]
      [<a href="materials/Gensim%20word%20vector%20visualization.html">preview</a>]-->
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> (original word2vec paper)</li>
        <li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a> (negative sampling paper)</li>
      </ol>
    </td>
    <td>
      Assignment 1 <b><font color="green">out</font></b>
      <br>
      [<a href="assignments/a1.zip">code</a>]
      <br>
      [<a href="assignments/a1_preview/exploring_word_vectors.html">preview</a>]
    </td>
    <td></td>
  </tr>

  <tr>
    <td>Thu Apr 4</td>
    <td>Word Vectors and Language Models
      <br>
      [<a href="slides/cs224n-spr2024-lecture02-wordvecs2.pdf">slides</a>]
      [<a href="readings/cs224n-2019-notes02-wordvecs2.pdf">notes</a>]
      [<a href="materials/gensim_2024.zip">code</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a> (original GloVe paper)</li>
        <li><a href="http://www.aclweb.org/anthology/Q15-1016">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
        <li><a href="http://www.aclweb.org/anthology/D15-1036">Evaluation methods for unsupervised word embeddings</a></li>
      </ol>
      Additional Readings:
      <ol>
        <li><a href="http://aclweb.org/anthology/Q16-1028">A Latent Variable Model Approach to PMI-based Word Embeddings</a></li>
        <li><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320">Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a></li>
        <li><a href="https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf">On the Dimensionality of Word Embedding</a></li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr class="warning">
    <td>Fri Apr 5</td>
    <td>Python Review Session
      <br>
      [<a href="readings/cs224n-python-review-2023.pdf">slides</a>]
      [<a href="https://colab.research.google.com/drive/1hxWtr98jXqRDs_rZLZcEmX_hUcpDLq6e?usp=sharing">colab</a>]
    </td>
    <td>
      <i class="fa fa-clock-o"></i> 3:30pm - 4:20pm <br> Gates B01
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td><b><font color="#8c1515">Week 2</font></b><br><br>Tue Apr 9</td>
    <td>Backpropagation and Neural Network Basics
      <br>
      [<a href="slides/cs224n-spr2024-lecture03-neuralnets.pdf">slides</a>]
      [<a href="readings/cs224n-2019-notes03-neuralnets.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="readings/gradient-notes.pdf">matrix calculus notes</a></li>
        <li><a href="readings/review-differential-calculus.pdf">Review of differential calculus</a></li>
        <li><a href="http://cs231n.github.io/neural-networks-1/">CS231n notes on network architectures</a></li>
        <li><a href="http://cs231n.github.io/optimization-2/">CS231n notes on backprop</a></li>
        <li><a href="http://cs231n.stanford.edu/handouts/derivatives.pdf">Derivatives, Backpropagation, and Vectorization</a></li>
        <li><a href="http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">Learning Representations by Backpropagating Errors</a> (seminal Rumelhart et al. backpropagation paper)</li>
      </ol>
      Additional Readings:
      <ol>
        <li><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand backprop</a></li>
        <li><a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">Natural Language Processing (Almost) from Scratch</a></li>
      </ol>
    </td>
    <td>
      Assignment 2 <b><font color="green">out</font></b>
      <br>
      [<a href="assignments/a2.zip">code</a>]
      <br>
      [<a href="assignments/a2.pdf">handout</a>]
      <br>
      [<a href="assignments/a2_latex_template.zip">latex template</a>]
    </td>
    <td>Assignment 1 <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td>Thu Apr 11</td>
    <td>Dependency Parsing
      <br>
      [<a href="slides/cs224n-spr2024-lecture04-dep-parsing.pdf">slides</a>]
      [<a href="readings/cs224n-2019-notes04-dependencyparsing.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.aclweb.org/anthology/W/W04/W04-0308.pdf">Incrementality in Deterministic Dependency Parsing</a></li>
        <li><a href="https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a></li>
        <li><a href="https://link.springer.com/book/10.1007/978-3-031-02131-2">Dependency Parsing</a></li>
        <li><a href="https://arxiv.org/pdf/1603.06042.pdf">Globally Normalized Transition-Based Neural Networks</a></li>
        <li><a href="http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf">Universal Stanford Dependencies: A cross-linguistic typology</a></li>
        <li><a href="http://universaldependencies.org/">Universal Dependencies website</a></li>
        <li><a href="https://web.stanford.edu/~jurafsky/slpdraft/18.pdf">Jurafsky & Martin Chapter 18</a></li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr class="warning">
    <td>Fri Apr 12</td>
    <td>PyTorch Tutorial Session
      <br>[<a href="https://colab.research.google.com/drive/1Pz8b_h-W9zIBk1p2e6v-YFYThG1NkYeS?usp=sharing">colab</a>]
    </td>
    <td>
      <i class="fa fa-clock-o"> </i> 3:30pm - 4:20pm <br> Gates B01
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td><b><font color="#8c1515">Week 3</font></b><br><br>Tue Apr 16</td>
    <td>Recurrent Neural Networks
      <br>
      [<a href="slides/cs224n-spr2024-lecture05-rnnlm.pdf">slides</a>]
      [<a href="readings/cs224n-2019-notes05-LM_RNN.pdf">notes (lectures 5 and 6)</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models</a> (textbook chapter)</li>
        <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> (blog post overview)</li>
        <!-- <li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Recurrent Neural Networks Tutorial</a> (practical guide)</li> -->
        <li><a href="http://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets</a> (Sections 10.1 and 10.2)</li>
        <li><a href="http://norvig.com/chomsky.html">On Chomsky and the Two Cultures of Statistical Learning</a>
        <li><a href="http://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets</a> (Sections 10.3, 10.5, 10.7-10.12)</li>
        <li><a href="https://ieeexplore.ieee.org/document/279181">Learning long-term dependencies with gradient descent is difficult</a> (one of the original vanishing gradient papers)</li>
        <li><a href="https://arxiv.org/pdf/1211.5063.pdf">On the difficulty of training Recurrent Neural Networks</a> (proof of vanishing gradient problem)</li>
        <li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html">Vanishing Gradients Jupyter Notebook</a> (demo for feedforward networks)</li>
        <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (blog post overview)</li>
        <!-- <li><a href="https://arxiv.org/pdf/1504.00941.pdf">A simple way to initialize recurrent networks of rectified linear units</a></li> -->
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Apr 18</td>
    <td>Sequence to Sequence Models and Machine Translation
      <br>
      [<a href="slides/cs224n-spr2024-lecture06-fancy-rnn.pdf">slides</a>]
      [<a href="readings/cs224n-2019-notes05-LM_RNN.pdf">notes (lectures 5 and 6)</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/syllabus.shtml">Statistical Machine Translation slides, CS224n 2015</a> (lectures 2/3/4)</li>
        <li><a href="https://www.cambridge.org/core/books/statistical-machine-translation/94EADF9F680558E13BE759997553CDE5">Statistical Machine Translation</a> (book by Philipp Koehn)</li>
        <li><a href="https://www.aclweb.org/anthology/P02-1040.pdf">BLEU</a> (original paper)</li>
        <li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to Sequence Learning with Neural Networks</a> (original seq2seq NMT paper)</a></li>
        <li><a href="https://arxiv.org/pdf/1211.3711.pdf">Sequence Transduction with Recurrent Neural Networks</a> (early seq2seq speech recognition paper)</li>
        <li><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a> (original seq2seq+attention paper)</li>
        <li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a> (blog post overview)</li>
        <li><a href="https://arxiv.org/pdf/1703.03906.pdf">Massive Exploration of Neural Machine Translation Architectures</a> (practical advice for hyperparameter choices)</li>
        <li><a href="https://arxiv.org/abs/1604.00788.pdf">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</a></li>
        <li><a href="https://arxiv.org/pdf/1808.09943.pdf">Revisiting Character-Based Neural Machine Translation with Capacity and Compression</a></li>
      </ol>
    </td>
    <td>Assignment 3 <b><font color="green">out</font></b>
      <br>
      [<a href="assignments/a3_spr24_student_code.zip">code</a>]
      <br>
      [<a href="assignments/a3_spr24_student_handout.pdf">handout</a>]
      <br>
      [<a href="assignments/a3_spr24_student_latex.zip">latex template</a>]
      <br>
      [<a href="https://www.overleaf.com/read/cmyyxpbfzqyr#c58d0d">overleaf link</a>]
    </td>
    <td>Assignment 2 <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td><b><font color="#8c1515">Week 4</font></b><br><br>Tue Apr 23</td>
    <td>Final Projects and LLM intro
      <br>
      [<a href="slides/cs224n-spr2024-lecture07-final-project.pdf">slides</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.deeplearningbook.org/contents/guidelines.html">Practical Methodology</a> (<i>Deep Learning</i> book chapter)</li>
      </ol>
    </td>
    <td>Project Proposal <b><font color="green">out</font></b>
      <br>
      [<a href="project/project-proposal-instructions-spr2024-updated.pdf">handout</a>]
      <br><br>
      Default Final Project <b><font color="green">out</font></b>
      <br>
      [<a href="project/default-final-project-handout-minbert-spr2024-updated.pdf">handout</a>]
    </td>
    <td></td>
  </tr>

  <tr>
    <td>Thu Apr 25</td>
    <td>Transformers
      <br>
      (<i>by Anna Goldie</i>)
      <br>
      [<a href="slides/cs224n-spr2024-lecture08-transformers.pdf">slides</a>]
      <!-- [<a href="project/custom-final-project-tips.pdf">Custom project tips</a>] -->
      [<a href="readings/cs224n-self-attention-transformers-2023_draft.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://arxiv.org/abs/1706.03762.pdf">Attention Is All You Need</a>
        </li>
        <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
        </li>
        <li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer (Google AI blog post)</a>
        </li>
        <li><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a></li>
        <li><a href="https://arxiv.org/pdf/1802.05751.pdf">Image Transformer</a></li>
        <li><a href="https://arxiv.org/pdf/1809.04281.pdf">Music Transformer: Generating music with long-term structure</a></li>
        <li><a href="https://web.stanford.edu/~jurafsky/slpdraft/10.pdf">Jurafsky and Martin Chapter 10 (Transformers and Large Language Models)</a></li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td><b><font color="#8c1515">Week 5</font></b><br><br>Tue Apr 30</td>
    <td>Pretraining
    <br>
    [<a href="slides/cs224n-spr2024-lecture09-pretraining-updated.pdf">slides</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li>
          <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
        </li>
        <li>
           <a href="https://arxiv.org/abs/1902.06006.pdf">Contextual Word Representations: A Contextual Introduction</a>
        </li>
        <li><a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a></li>
        <li><a href="https://web.stanford.edu/~jurafsky/slpdraft/11.pdf">Jurafsky and Martin Chapter 11 (Fine-Tuning and Masked Language Models)</a></li>
      </ol>
    </td>
    <td>Assignment 4 <b><font color="green">out</font></b>
      <br>
      [<a href="assignments/a4_spr24_student_code.zip">code</a>]
      <br>
      [<a href="assignments/a4_spr24_student_handout.pdf">handout</a>]
      <br>
      [<a href="https://www.overleaf.com/read/srrtnkygvdpj#fc4729">overleaf</a>]
      <br>
      [<a href="https://colab.research.google.com/drive/193j033YPWyzooOK_C4tSBpnaguoewN7X?usp=sharing">colab run script</a>]
    </td>
    <td>Assignment 3 <b><font color="red">due</font></b></td>
  </tr>

  <tr>
    <td>Thu May 2</td>
    <td> Post-training (RLHF, SFT, DPO)
      <br>
      (<i>by Archit Sharma</i>)
      <br>
      [<a href="slides/cs224n-spr2024-lecture10-prompting-rlhf.pdf">slides</a>]
    </td>
    <td>
    Suggested Readings:
      <ol>
        <li>
          <a href="https://openai.com/research/instruction-following">Aligning language models to follow instructions</a>
          </li>
          <li>
          <a href="https://arxiv.org/abs/2210.11416">Scaling Instruction-Finetuned Language Models</a>
          </li>
          <li>
          <a href="https://arxiv.org/abs/2305.14387">AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</a>
          </li>
          <li>
          <a href="https://arxiv.org/abs/2306.04751">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</a>
          </li>
          <li>
          <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>
          </li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>

  <!-- TODO: Add session room and time -->
  <tr class="warning">
    <td>Fri May 3</td>
    <td>Hugging Face Transformers Tutorial Session
      <br>
      [<a href="https://colab.research.google.com/drive/13r94i6Fh4oYf-eJRSi7S_y_cen5NYkBm#scrollTo=OTsW-Wwi-X81">colab</a>]
    </td>
    <td>
      <i class="fa fa-clock-o"> </i> 3:30pm - 4:20pm <br> Gates B03
    </td>
    <td></td>
    <td>Project Proposal <b><font color="red">due</font></b><br></td>
  </tr>
  <!-- TODO: Update lecture readings and slides-->
  <tr>
    <td><b><font color="#8c1515">Week 6</font></b><br><br>Tue May 7</td>
    <td> Benchmarking and Evaluation
      <br>
      (<i>by Yann Dubois</i>)
      <br>
      [<a href="slides/cs224n-spr2024-lecture11-evaluation-yann.pdf">slides</a>]
    </td>
    <td>
    Suggested Readings:
      <ol>
        <li>
        <a href="https://www.ruder.io/nlp-benchmarking/">Challenges and Opportunities in NLP Benchmarking</a>
        </li>
        <li>
        <a href="https://arxiv.org/abs/2009.03300">Measuring Massive Multitask Language Understanding</a>
        </li>
        <li>
        <a href="https://arxiv.org/abs/2211.09110">Holistic Evaluation of Language Models</a>
        </li>
        <li>
        <a href="https://tatsu-lab.github.io/alpaca_eval/">AlpacaEval</a>
        </li>
      </ol>
    </td>
    <td></td>
    <td></td>
  </tr>
  <!-- TODO: Update lecture readings and slides-->
  <tr>
    <td>Thu May 9</td>
    <td> Efficient Neural Network Training
      <br>
      (<i>by Shikhar Murty</i>)
      <br>
      [<a href="slides/cs224n-spr2024-lecture12-training-shikhar.pdf">slides</a>]
    </td>
    <td>
    Suggested readings:
      <ol>
        <li>
          <a href="https://arxiv.org/abs/1710.03740">Mixed Precision Training</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/1910.02054">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2304.11277">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a>
        </li>
      </ol>

      <!-- <ol>
        <li>
          <a href="https://arxiv.org/abs/1606.05250">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/1611.01603">Bidirectional Attention Flow for Machine Comprehension</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/1704.00051">Reading Wikipedia to Answer Open-Domain Questions</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2002.08909">REALM: Retrieval-Augmented Language Model Pre-Training</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a>
        </li>
      </ol> -->
    </td>
    <td>Final Project Proposals <b><font color="blue">Returned</font></b><br>
      <br>Project Milestone <b><font color="green">out</font></b>
      <br>
      [<a href="project/project-milestone-instructions-spr2024.pdf">handout</a>]
    </td>
    <td>Assignment 4 <b><font color="red">due</font></b></td>
  </tr>

  <!-- TODO: Update lecture readings and slides-->
  <tr>
    <td><b><font color="#8c1515">Week 7</font></b><br><br>Tue May 14</td>
      <td>Speech Brain-Computer Interface
        <br>
        (<i>by Chaofei Fan</i>)
        <br>
        [<a href="slides/cs224n-spr2024-lecture13-speech-bci.pdf">slides</a>]
      </td>
    <td>
      Suggested readings:
      <ol>
        <li><a href="https://www.nature.com/articles/s41586-023-06377-x">A high-performance speech neuroprosthesis</a></li>
        <li><a href="https://www.medrxiv.org/content/10.1101/2023.12.26.23300110v2">An accurate and rapidly calibrating speech neuroprosthesis</a></li>
        <li><a href="https://www.nature.com/articles/s41586-023-06443-4">A high-performance neuroprosthesis for speech decoding and avatar control</a></li>
        <li><a href="https://users.ece.cmu.edu/~byronyu/papers/PNS-6thEdition-SectionV-Motor-Chapter39-BMIs.pdf">Brain-Machine Interfaces (Principles of Neural Science chapter)</a></li>
      </ol>
      <!-- <ol>
        <li><a href="https://arxiv.org/abs/2012.07805">Extracting Training Data from Large Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2311.17035">Scalable Extraction of Training Data from (Production) Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2202.07646">Quantifying Memorization Across Neural Language Models</a></li>
      </ol> -->
      <!-- <ol>
        <li><a href="https://arxiv.org/abs/1408.5882.pdf">Convolutional Neural Networks for Sentence Classification</a></li>
        <li><a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
        <li><a href="https://arxiv.org/pdf/1404.2188.pdf">A Convolutional Neural Network for Modelling Sentences</a></li>
        <li><a href="http://www.aclweb.org/anthology/P13-1045">Parsing with Compositional Vector Grammars.</a></li>
        <li><a href="https://arxiv.org/pdf/1805.01052.pdf">Constituency Parsing with a Self-Attentive Encoder</a></li>
      </ol>-->
    </td>
    <td></td>
    <td></td>
  </tr>


  <!-- TODO: Update lecture readings and slides-->
  <tr>
    <td>Thu May 16</td>
    <td>Reasoning and Agents
      <br>
      (<i>by Shikhar Murty</i>)
      <br>
      [<a href="slides/cs224n-spr2024-lecture14-agents-shikhar-updated.pdf">slides</a>]
    </td>
    <td>
    Suggested readings:
    <ol>
      <li><a href="https://arxiv.org/abs/2306.02707">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></li>
      <li><a href="https://arxiv.org/abs/2205.10625">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a></li>
      <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
      <li><a href="https://arxiv.org/abs/2403.08140">BAGEL: Bootstrapping Agents by Guiding Exploration with Language</a></li>
      <li><a href="https://arxiv.org/abs/2307.13854">WebArena: A Realistic Web Environment for Building Autonomous Agents</a></li>
    </ol>

    Additional Readings:
    <ol>
      <li><a href="https://arxiv.org/abs/2307.02477">Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</a></li>
      <li><a href="https://arxiv.org/abs/2308.16118">Response: Emergent analogical reasoning in large language models</a></li>
      <li><a href="https://arxiv.org/abs/2402.05930">WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</a></li>
    </ol>

    <!-- <ol>
      <li><a href="https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf">The Risk of Racial Bias in Hate Speech Detection</a></li>
      <li><a href="https://homes.cs.washington.edu/~msap/social-bias-frames/">Social Bias Frames</a></li>
      <li><a href="https://arxiv.org/abs/2010.13816">PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction</a></li>
    </ol> -->
    </td>
    <td></td>
    <td></td>
  </tr>

  <!-- TODO: Add guest speaker info -->
  <tr>
    <td><b><font color="#8c1515">Week 8</font></b><br><br>Tue May 21</td>
    <td>Life after DPO
      <br>
      (<i>by Nathan Lambert</i>)
      <br>
      [<a href="slides/cs224n-spr2024-lecture15-life-after-dpo-lambert.pdf">slides</a>]<br>
    </td>
    <td>
    Suggested readings:
    <ol>
      <li><a href="https://arxiv.org/abs/2403.13787">RewardBench: Evaluating Reward Models for Language Modeling</a></li>
      <li><a href="https://arxiv.org/abs/2405.01511">D2PO: Discriminator-Guided DPO with Response Evaluation Models</a></li>
      <li><a href="https://arxiv.org/abs/2404.10271">Social Choice for AI Alignment: Dealing with Diverse Human Feedback</a></li>
    </ol>
    <!-- <ol>
      <li><a href="https://arxiv.org/pdf/2108.07732.pdf">Program Synthesis with Large Language Models</a></li>
      <li><a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a></li>
      <li><a href="https://arxiv.org/abs/2308.12950">Code Llama: Open Foundation Models for Code</a></li>
      <li><a href="https://arxiv.org/abs/2212.09248">Natural Language to Code Generation in Interactive Data Science Notebooks</a></li>
      <li><a href="https://arxiv.org/abs/2304.05128">Teaching Large Language Models to Self-Debug</a></li>
      <li><a href="https://arxiv.org/abs/2402.08699">Unsupervised Evaluation of Code LLMs with Round-Trip Correctness</a></li>
    </ol> -->
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr class="warning">
    <td>Wed May 22</td>
    <td></td>
    <td></td>
    <td></td>
    <td>Final Project Milestone <b><font color="red">due</font></b></td>
  </tr>

  <!-- TODO: Update lecture readings and slides-->
  <tr>
    <td>Thu May 23</td>
    <td>ConvNets, Tree Recursive Neural Networks and Constituency Parsing
    <br>
    [<a href="slides/cs224n-spr2024-lecture16-CNN-TreeRNN.pdf">slides</a>]
    </td>
    <td>
      Suggested readings (tentative):
      <!-- <ol>
        <li><a href="https://arxiv.org/abs/2201.07520">CM3: A Causal Masked Multimodal Model of the Internet</a></li>
        <li><a href="https://arxiv.org/abs/2309.02591">Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning</a></li>
        <li><a href="https://arxiv.org/abs/2301.03728">Scaling Laws for Generative Mixed-Modal Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2309.14322">Small-scale proxies for large-scale Transformer training instabilities</a></li>
      </ol> -->
      <ol>
        <li><a href="https://arxiv.org/abs/1408.5882.pdf">Convolutional Neural Networks for Sentence Classification</a></li>
        <li><a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
        <li><a href="https://arxiv.org/pdf/1404.2188.pdf">A Convolutional Neural Network for Modelling Sentences</a></li>
        <li><a href="http://www.aclweb.org/anthology/P13-1045">Parsing with Compositional Vector Grammars.</a></li>
        <li><a href="https://arxiv.org/pdf/1805.01052.pdf">Constituency Parsing with a Self-Attentive Encoder</a></li>
      </ol>
    </td>
    <td>Final Project Report Instructions <b><font color="green">out</font></b>
      <br>[<a href="project/CS224n_Spring_2024_Final_Project_Report_Instructions.pdf">Instructions</a>]
    </td>
    <td></td>
  </tr>

  <tr class="warning">
    <td>Fri May 24</td>
    <td></i>
    <br>
  </td>
    <td>
    </td>
    <td></td>
    <td>Course Withdrawal <b><font color="red">Deadline</font></b></td>
  </tr>

  <!-- TODO: Add guest speaker info -->
  <tr>
    <td><b><font color="#8c1515">Week 9</font></b><br><br>Tue May 28</td>
    <td>An Introduction to Responsible NLP
      <br>
      (<i>by Adina Williams</i>)
      <!-- <br>
      [<a href="slides/cs224n-2024-lecture17-human-centered-nlp.pdf">slides</a>] -->
    </td>
    <td>Suggested readings:
      <ol>
        <li><a href="https://fairmlbook.org/pdf/fairmlbook.pdf"> Preface + Introduction chapter of the FairML book by Solon Barocas, Moritz Hardt, Arvind Narayanan</a></li>
        <li><a href="https://arxiv.org/abs/2404.12241">Introducing v0.5 of the AI Safety Benchmark from MLCommons</a></li>
      </ol>
      <!-- <ol>
        <li><a href="https://arxiv.org/abs/2205.06905">Perspectives on Incorporating Expert Feedback into Model Updates</a></li>
        <li><a href="https://arxiv.org/abs/2106.10328">Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets</a></li>
        <li><a href="https://arxiv.org/abs/2212.08011">Multi-VALUE: A Framework for Cross-Dialectal English NLP</a></li>
        <li><a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback</a></li>
        <li><a href="https://arxiv.org/abs/2310.15428">ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles</a></li>
      </ol> -->
    </td>
    <td>Final Project Milestones <b><font color="blue">Returned</font></b></td>
    <td></td>
  </tr>

  <tr>
    <td>Thu May 30</td>
    <td>NLP, linguistics, and philosophy
      <br>
      [<a href="slides/cs224n-spr2024-lecture18-nlp-linguistics-philosophy.pdf">slides</a>]
    </td>
    <td>
      Suggested readings:
      <!-- <ol>
        <li><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
        </a></li>
        <li><a href="https://github.com/srush/do-we-need-attention/blob/main/DoWeNeedAttention.pdf">Do we need Attention?</a></li>
      </ol> -->
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td><b><font color="#8c1515">Week 10</font></b><br><br>Tue June 4</td>
    <td>Final Project Emergency Assistance (no lecture)
    </td>
    <td>Extra project office hours available during usual lecture time, see Ed.</td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Thu June 6</td>
    <td>No class
    </td>
    <td></td>
    <td></td>
    <td>Final project <b><font color="red">due</font></b>
    </td>
  </tr>

  <tr class="warning">
    <td>Mon June 10</td>
    <td>Final Project Poster Session
      <!--<br>
      [<a href="readings/cs224n-python-review-code-updated.zip">code</a>]
      [<a href="readings/cs224n-python-review-code-updated.pdf">preview</a>]-->
    </td>
    <td>
      <i class="fa fa-clock-o"></i> 11 am - 3 pm [<a href="project.html">More details</a>]<br>
      Location: <a href="https://maps.app.goo.gl/MYgtYPLv61NKsj3G6">McCaw Hall and Ford Gardens</a> <br>
      On-campus students must attend in person!
    </td>
    <td></td>
    <td>[<a href="https://docs.google.com/document/d/1J8-TVVvndimSwq3jGzpMO_OtPAx_EaU9nUiiPUQdVpY">Printing guide</a>]</td>
  </tr>

  </tbody>
</table>
</div>

</body>

</html>
